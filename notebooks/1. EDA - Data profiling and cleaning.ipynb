{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c66022",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb8dc1",
   "metadata": {},
   "source": [
    "There are various analysis we can conduct on the same dataset. It is important to set the agenda before looking at the data to ensure we do not get lost along the way.\n",
    "\n",
    "For this analysis, we are interested in understanding the key factors that affects the `resale price`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ed337",
   "metadata": {},
   "source": [
    "# Retrieve data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1916500",
   "metadata": {},
   "source": [
    "We retrieve data from Data.gov.sg.\n",
    "\n",
    "We can manually download the data, here we implemented a custom object to download data with a given URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560fc8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:19.300978Z",
     "start_time": "2021-07-17T04:09:18.100131Z"
    }
   },
   "outputs": [],
   "source": [
    "###############\n",
    "# import data #\n",
    "###############\n",
    "import numpy as np  # numeric operations\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import pandas as pd  # dataframe\n",
    "import pickle  # read stored location data\n",
    "\n",
    "import gevent.monkey\n",
    "gevent.monkey.patch_all()\n",
    "\n",
    "from hdb_resale_data import (\n",
    "    # retrive location data\n",
    "    Location,\n",
    "    # custom Data object\n",
    "    Data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a7c79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:34.337980Z",
     "start_time": "2021-07-17T04:09:19.329294Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://data.gov.sg/dataset/7a339d20-3c57-4b11-a695-9348adfd7614/download\"\n",
    "data = Data(url)  # Data takes in a url: string\n",
    "\n",
    "data.download(filename=\"../data/data.zip\")  # Data stores the downloaded object with the given filename\n",
    "data.zip_filename(zip_file=\"../data/data.zip\")  # display the file names we downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006fc34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:34.579522Z",
     "start_time": "2021-07-17T04:09:34.365754Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the file inside the zip file\n",
    "df = data.read_zip(zip_file=\"../data/data.zip\", \n",
    "                   filename=\"resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e28232",
   "metadata": {},
   "source": [
    "# Data profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c832b0",
   "metadata": {},
   "source": [
    "We want to check for the following profiles:\n",
    "\n",
    "1. Missing values\n",
    "2. Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e4c8c",
   "metadata": {},
   "source": [
    "We will be keeping a profile of the list of data cleaning steps required before conducting data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7aa9b7",
   "metadata": {},
   "source": [
    "## Data preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c82a43",
   "metadata": {},
   "source": [
    "Sometimes the easiest way to identify data cleaning steps is to look at the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93e50e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:34.680088Z",
     "start_time": "2021-07-17T04:09:34.609575Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc43a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:34.774236Z",
     "start_time": "2021-07-17T04:09:34.695537Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00172810",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "1. `month` is coded as object (string) and not datetime\n",
    "2. `remaining_lease` is coded as object (string) and not datetime\n",
    "3. Location data is in string, hard to have any meaningful interpretation\n",
    "4. `storey_range` is in string, hard to have any meaningful interpretation\n",
    "5. `flat_type` is in string, hard to have any meaningful interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84791544",
   "metadata": {},
   "source": [
    "Let's fix the issues before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be04467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:34.883165Z",
     "start_time": "2021-07-17T04:09:34.810667Z"
    }
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# fix datetime for month #\n",
    "##########################\n",
    "# problem: month is in yyyy-mm, common datetime format require a day as well\n",
    "# solution: concatenate a string '-01' before converting to datetime\n",
    "# in order to ensure pandas convert our datetime object correctly,\n",
    "# we will explictly input the format\n",
    "df[\"month\"] = pd.to_datetime(df[\"month\"] + \"-01\", format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32debe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:35.282737Z",
     "start_time": "2021-07-17T04:09:34.912585Z"
    }
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "# fix datetime for remaining_lease #\n",
    "####################################\n",
    "# problem: remaining_lease is in years and month (string), we want a standardised unit\n",
    "# solution: convert remaining_lease to years (year = month/12)\n",
    "# we use Regex to extract out the years and month\n",
    "years = df[\"remaining_lease\"].str.extract(\"(\\d+) years\").astype(\"float\")\n",
    "months = df[\"remaining_lease\"].str.extract(\"years (\\d+) [months]|[month]\").astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d1473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:35.354503Z",
     "start_time": "2021-07-17T04:09:35.309492Z"
    }
   },
   "outputs": [],
   "source": [
    "# we have some missing months, let's make sure those are entries without a month data\n",
    "df.loc[months.isna().values, \"remaining_lease\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a637338",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:35.412637Z",
     "start_time": "2021-07-17T04:09:35.382285Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill missing months values with 0\n",
    "months.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21fdac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:35.478319Z",
     "start_time": "2021-07-17T04:09:35.444890Z"
    }
   },
   "outputs": [],
   "source": [
    "# add a new column with remaining lease in years\n",
    "# and remove the old column\n",
    "df[\"remaining_lease_years\"] = years + months/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b4a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:38.770029Z",
     "start_time": "2021-07-17T04:09:35.507449Z"
    }
   },
   "outputs": [],
   "source": [
    "#####################\n",
    "# fix location data #\n",
    "#####################\n",
    "# problem: location data is in string, it is hard to compare against different locations\n",
    "# solution: retrive geolocation\n",
    "# retriving geolocation is a more challenging task, we illustrate the idea here and\n",
    "# execute it in a seperate script\n",
    "loc = Location()\n",
    "location = df[\"block\"] + \" \" + df[\"street_name\"]\n",
    "links = loc.url1 + location + loc.url2\n",
    "responses = loc.get_response(links.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e28b69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:38.899113Z",
     "start_time": "2021-07-17T04:09:38.797771Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"street_name\"].str.replace(\"ST.\", \"ST\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8315efcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:38.993138Z",
     "start_time": "2021-07-17T04:09:38.930556Z"
    }
   },
   "outputs": [],
   "source": [
    "# illustrate an example here\n",
    "responses.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6761365",
   "metadata": {},
   "source": [
    "We will complete the location request in our data cleaning step as it is time consuming.\n",
    "\n",
    "run `ETL.py` to repeat the data preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e8edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:39.978892Z",
     "start_time": "2021-07-17T04:09:39.021763Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../data/response\", \"rb\") as f:\n",
    "    responses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9392a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:40.285865Z",
     "start_time": "2021-07-17T04:09:40.080495Z"
    }
   },
   "outputs": [],
   "source": [
    "# we have every row of data\n",
    "len(responses), df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693ac31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:40.487476Z",
     "start_time": "2021-07-17T04:09:40.384286Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_location(response_obj):\n",
    "    address = []\n",
    "    postal = []\n",
    "    latitude = []\n",
    "    longtitude = []\n",
    "    for content in iter(response_obj):\n",
    "        try:\n",
    "            results = content[\"results\"]\n",
    "            # take only the first result\n",
    "            result = results[0]\n",
    "            address.append(result[\"ADDRESS\"])\n",
    "            postal.append(result[\"POSTAL\"])\n",
    "            latitude.append(result[\"LATITUDE\"])\n",
    "            longtitude.append(result[\"LONGTITUDE\"])\n",
    "        except:\n",
    "            address.append(None)\n",
    "            postal.append(None)\n",
    "            latitude.append(None)\n",
    "            longtitude.append(None)\n",
    "    return address, postal, latitude, longtitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b78e88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:40.784662Z",
     "start_time": "2021-07-17T04:09:40.587704Z"
    }
   },
   "outputs": [],
   "source": [
    "address, postal, latitude, longtitude = get_location(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb7371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:41.038539Z",
     "start_time": "2021-07-17T04:09:40.883287Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"address\"] = address\n",
    "df[\"postal\"] = postal\n",
    "df[\"latitude\"] = latitude\n",
    "df[\"longtitude\"] = longtitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcca16c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:41.364841Z",
     "start_time": "2021-07-17T04:09:41.139768Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's check if address are input correctly\n",
    "# we observe that only ST. GEORGE'S is not able to retrive any data\n",
    "# from our detailed analysis, we realise it's because of the full stop (.)\n",
    "# most likely REST API interpret it as a reserved keyword\n",
    "df.loc[df[\"address\"].isna(), \"street_name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63429799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:41.581197Z",
     "start_time": "2021-07-17T04:09:41.465978Z"
    }
   },
   "outputs": [],
   "source": [
    "# corrected ST. to ST\n",
    "edited_subset_links = links[df[\"address\"].isna()].replace(\"ST\\.\", \"ST\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f577587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:09:41.792449Z",
     "start_time": "2021-07-17T04:09:41.686900Z"
    }
   },
   "outputs": [],
   "source": [
    "# there could be some network issue resulting in address failed to fetch\n",
    "# we will iterate the replacement for a pre-defined number of tries\n",
    "max_try = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac066b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:10:59.724399Z",
     "start_time": "2021-07-17T04:09:41.892588Z"
    }
   },
   "outputs": [],
   "source": [
    "while df[\"address\"].isna().sum() > 0 and max_try > 0:\n",
    "    print(f\"==========try {max_try}============\")\n",
    "    edited_subset_links = links[df[\"address\"].isna()].replace(\"ST\\.\", \"ST\", regex=True)\n",
    "    subset_response = loc.get_gresponse(edited_subset_links)\n",
    "    subset_address = []\n",
    "    for response in iter(subset_response):\n",
    "        if response:\n",
    "            subset_address.append(response.json())\n",
    "        else:\n",
    "            subset_address.append(None)\n",
    "    subset_address, subset_postal, subset_latitude, subset_longtitude = get_location(subset_address)\n",
    "    df.loc[df[\"address\"].isna(), \"address\"] = subset_address\n",
    "    df.loc[df[\"postal\"].isna(), \"postal\"] = subset_postal\n",
    "    df.loc[df[\"latitude\"].isna(), \"latitude\"] = subset_latitude\n",
    "    df.loc[df[\"longtitude\"].isna(), \"longtitude\"] = subset_longtitude\n",
    "    \n",
    "    max_try -= 1 # decrease try by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ef0a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:00.316876Z",
     "start_time": "2021-07-17T04:10:59.914850Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"address\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e6505d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:01.026842Z",
     "start_time": "2021-07-17T04:11:00.468219Z"
    }
   },
   "outputs": [],
   "source": [
    "####################\n",
    "# fix storey_range #\n",
    "####################\n",
    "# problem: storey_range is in string, it is hard to have meaning comparision\n",
    "# solution: since the storey range is a numeric variable, let's take the first storey\n",
    "df[\"storey_min\"] = df[\"storey_range\"].str.extract(\"(\\d+) TO\")\n",
    "df[\"storey_min\"] = df[\"storey_min\"].astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28976c54",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876c6ce",
   "metadata": {},
   "source": [
    "As we have illustrated with the Data Preview section, simply by looking at the data and understanding\n",
    "the data provides with us much things to do for data cleaning.\n",
    "\n",
    "Now, let's remove the columns that is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0302ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:01.368743Z",
     "start_time": "2021-07-17T04:11:01.177317Z"
    }
   },
   "outputs": [],
   "source": [
    "df.pop(\"storey_range\");\n",
    "df.pop(\"remaining_lease\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f80887",
   "metadata": {},
   "source": [
    "Now, notice although we have recoded the address but we did not remove the relevant columns.\n",
    "\n",
    "We could technically remove those columns, but I have a feeling it might come in useful in the future.\n",
    "\n",
    "Let's keep the columns for now, but have in mind that those columns are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2b639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:01.740719Z",
     "start_time": "2021-07-17T04:11:01.444370Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504227f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:02.193271Z",
     "start_time": "2021-07-17T04:11:01.886357Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71ffdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:02.577043Z",
     "start_time": "2021-07-17T04:11:02.352639Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d894a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:03.092544Z",
     "start_time": "2021-07-17T04:11:02.722743Z"
    }
   },
   "outputs": [],
   "source": [
    "df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade1c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:11:03.577643Z",
     "start_time": "2021-07-17T04:11:03.239407Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will save the data for our exploration next\n",
    "# we will pickle the df to preserve the datatypes\n",
    "with open(\"../data/hdb_final\", \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
